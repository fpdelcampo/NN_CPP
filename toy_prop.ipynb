{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# network goes R50 --> R20 --> R20 --> R10\n",
    "w_1 = np.random.normal(0,0.05,(20, 50))\n",
    "b_1 = np.random.normal(0,0.05,(20, 1))\n",
    "\n",
    "w_2 = np.random.normal(0,0.05,(10, 20))\n",
    "b_2 = np.random.normal(0,0.05,(10,1))\n",
    "\n",
    "w_3 = np.random.normal(0,0.05,(1, 10))\n",
    "b_3 = np.random.normal(0,0.05,(1,1))\n",
    "\n",
    "X = np.random.normal(0, 1, (50, 10000))\n",
    "y = np.array([np.sum(X[:, x]**2) for x in range(10000)])\n",
    "\n",
    "X_train = X[:, :8000]\n",
    "X_test = X[:, 8000:]\n",
    "\n",
    "y_train = np.sum(X_train[:8000], axis=0).reshape(1, 8000)\n",
    "y_test = np.sum(X_test[8000:], axis=0).reshape(1, 2000)\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return .5*(y-y_pred)**2\n",
    "\n",
    "def dloss(y, y_pred):\n",
    "    return y-y_pred\n",
    "\n",
    "def activation(z):\n",
    "    return z * (z>0)\n",
    "\n",
    "def dactivation(z):\n",
    "    return 1 * (z>0)\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "def forward(X, params):\n",
    "    w_1, b_1, w_2, b_2, w_3, b_3 = params\n",
    "    z_1 = w_1 @ X + b_1\n",
    "    a_1 = activation(z_1)\n",
    "\n",
    "    z_2 = w_2 @ a_1 + b_2\n",
    "    a_2 = activation(z_2)\n",
    "\n",
    "    z_3 =  w_3 @ a_2 + b_3\n",
    "    a_3 = activation(z_3)\n",
    "\n",
    "    return a_3, a_2, a_1, z_3, z_2, z_1\n",
    "\n",
    "epochs = 100\n",
    "for _ in range(epochs):    \n",
    "    a_3, a_2, a_1, z_3, z_2, z_1 = forward(X_train, (w_1, b_1, w_2, b_2, w_3, b_3))\n",
    "    out_test, _, _, _, _, _ = forward(X_test, (w_1, b_1, w_2, b_2, w_3, b_3))\n",
    "    \n",
    "    l = loss(y_train, a_3)\n",
    "    \n",
    "    dl = dloss(y_train, a_3) # 1 x 8000 \n",
    "    \n",
    "    dZ3 = dl * dactivation(z_3) # 1 x 8000\n",
    "    dW3 = np.dot(dZ3, a_2.T) / X_train.shape[1]\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / X_train.shape[1]\n",
    "    \n",
    "    dA2 = np.dot(w_3.T, dZ3)\n",
    "    dZ2 = dA2 * dactivation(z_2)\n",
    "    dW2 = np.dot(dZ2, a_1.T) / X_train.shape[1]\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / X_train.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(w_2.T, dZ2)\n",
    "    dZ1 = dA1 * dactivation(z_1)\n",
    "    dW1 = np.dot(dZ1, X_train.T) / X_train.shape[1]\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / X_train.shape[1]\n",
    "\n",
    "    w_3 -= 0.01*dW3\n",
    "    w_2 -= 0.01*dW2\n",
    "    w_1 -= 0.01*dW1\n",
    "\n",
    "    b_3 -= 0.01*db3\n",
    "    b_2 -= 0.01*db2\n",
    "    b_1 -= 0.01*db1\n",
    "\n",
    "    train_losses.append(np.mean(l))\n",
    "    test_losses.append(np.mean(loss(y_test, out_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

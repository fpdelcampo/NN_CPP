{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,1) and (8000,8000) not aligned: 1 (dim 1) != 8000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/francisco/Code/NN_CPP/toy_prop.ipynb Cell 1\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francisco/Code/NN_CPP/toy_prop.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m dW3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dZ3, a_2\u001b[39m.\u001b[39mT) \u001b[39m/\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m# This should be 1 x 10\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francisco/Code/NN_CPP/toy_prop.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m db3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dZ3, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m/\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francisco/Code/NN_CPP/toy_prop.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m dA2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(w_3\u001b[39m.\u001b[39;49mT, dZ3)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francisco/Code/NN_CPP/toy_prop.ipynb#W0sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m dZ2 \u001b[39m=\u001b[39m dA2 \u001b[39m*\u001b[39m dactivation(z_2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francisco/Code/NN_CPP/toy_prop.ipynb#W0sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m dW2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dZ2, a_1\u001b[39m.\u001b[39mT) \u001b[39m/\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,1) and (8000,8000) not aligned: 1 (dim 1) != 8000 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# network goes R50 --> R20 --> R20 --> R10\n",
    "w_1 = np.random.normal(0,0.05,(20, 50))\n",
    "b_1 = np.random.normal(0,0.05,(20, 1))\n",
    "\n",
    "w_2 = np.random.normal(0,0.05,(10, 20))\n",
    "b_2 = np.random.normal(0,0.05,(10,1))\n",
    "\n",
    "w_3 = np.random.normal(0,0.05,(1, 10))\n",
    "b_3 = np.random.normal(0,0.05,(1,1))\n",
    "\n",
    "X = np.random.normal(0, 1, (50, 10000))\n",
    "y = np.array([np.sum(X[:, x]**2) for x in range(10000)])\n",
    "\n",
    "X_train = X[:, :8000]\n",
    "X_test = X[:, 8000:]\n",
    "\n",
    "y_train = y[:8000]\n",
    "y_test = y[8000:]\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return .5*np.square(y_pred-y)\n",
    "\n",
    "def dloss(y, y_pred):\n",
    "    return y_pred-y\n",
    "\n",
    "def activation(z):\n",
    "    return z * (z>0)\n",
    "\n",
    "def dactivation(z):\n",
    "    return 1 * (z>0)\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "def forward(X, params):\n",
    "    w_1, b_1, w_2, b_2, w_3, b_3 = params\n",
    "    z_1 = w_1 @ X + b_1\n",
    "    a_1 = activation(z_1)\n",
    "\n",
    "    z_2 = w_2 @ a_1 + b_2\n",
    "    a_2 = activation(z_2)\n",
    "\n",
    "    z_3 =  w_3 @ a_2 + b_3\n",
    "    a_3 = activation(z_3)\n",
    "\n",
    "    return a_3, a_2, a_1, z_3, z_2, z_1\n",
    "\n",
    "epochs = 100\n",
    "for _ in range(epochs):    \n",
    "    a_3, a_2, a_1, z_3, z_2, z_1 = forward(X_train, (w_1, b_1, w_2, b_2, w_3, b_3))\n",
    "    out_test, _, _, _, _, _ = forward(X_test, (w_1, b_1, w_2, b_2, w_3, b_3))\n",
    "    \n",
    "    l = loss(y_train, a_3)\n",
    "    \n",
    "    dl = dloss(y_train, a_3) # 1 x 8000 \n",
    "    \n",
    "    dZ3 = dl * dactivation(z_3) # 1 x 8000\n",
    "    dW3 = np.dot(dZ3, a_2.T) / X_train.shape[1] # This should be 1 x 10\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / X_train.shape[1]\n",
    "    \n",
    "    dA2 = np.dot(w_3.T, dZ3)\n",
    "    dZ2 = dA2 * dactivation(z_2)\n",
    "    dW2 = np.dot(dZ2, a_1.T) / X_train.shape[1]\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / X_train.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(w_2.T, dZ2)\n",
    "    dZ1 = dA1 * dactivation(z_1)\n",
    "    dW1 = np.dot(dZ1, X_train.T) / X_train.shape[1]\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / X_train.shape[1]\n",
    "\n",
    "    w_3 -= 0.01*dW3\n",
    "    w_2 -= 0.01*dW2\n",
    "    w_1 -= 0.01*dW1\n",
    "\n",
    "    b_3 -= 0.01*db3\n",
    "    b_2 -= 0.01*db2\n",
    "    b_1 -= 0.01*db1\n",
    "\n",
    "    train_losses.append(np.mean(l))\n",
    "    test_losses.append(np.mean(loss(y_test, out_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1412.0016\n",
      "Epoch 100, Loss: 49.9446\n",
      "Epoch 200, Loss: 49.2804\n",
      "Epoch 300, Loss: 48.9670\n",
      "Epoch 400, Loss: 48.7071\n",
      "Epoch 500, Loss: 48.4367\n",
      "Epoch 600, Loss: 48.1156\n",
      "Epoch 700, Loss: 47.7201\n",
      "Epoch 800, Loss: 47.2291\n",
      "Epoch 900, Loss: 46.6139\n",
      "Validation Loss: 46.0358\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "#X = np.random.rand(10000, 50)\n",
    "#y = np.random.rand(10000, 1)\n",
    "\n",
    "X = np.random.normal(0, 1, (100000,50))\n",
    "y = np.array([np.sum(X[x,:]) for x in range(100000)]).reshape(100000,1)\n",
    "\n",
    "# Split data into training and validation sets (80/20)\n",
    "split_idx = int(0.8 * X.shape[0])\n",
    "X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "X_val, y_val = X[split_idx:], y[split_idx:]\n",
    "\n",
    "# Neural network architecture\n",
    "input_size = 50\n",
    "hidden_size1 = 20\n",
    "hidden_size2 = 10\n",
    "output_size = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size1)\n",
    "b1 = np.zeros((1, hidden_size1))\n",
    "W2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "b2 = np.zeros((1, hidden_size2))\n",
    "W3 = np.random.randn(hidden_size2, output_size)\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = X_train.dot(W1) + b1\n",
    "    a1 = np.maximum(0, z1)  # ReLU activation\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.maximum(0, z2)  # ReLU activation\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    predictions = z3\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    loss = np.mean((predictions - y_train) ** 2)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_loss = 2 * (predictions - y_train) / X_train.shape[0]\n",
    "    grad_z3 = grad_loss\n",
    "    grad_W3 = a2.T.dot(grad_z3)\n",
    "    grad_b3 = np.sum(grad_z3, axis=0, keepdims=True)\n",
    "    grad_a2 = grad_z3.dot(W3.T)\n",
    "    grad_z2 = grad_a2 * (z2 > 0)  # ReLU gradient\n",
    "    grad_W2 = a1.T.dot(grad_z2)\n",
    "    grad_b2 = np.sum(grad_z2, axis=0, keepdims=True)\n",
    "    grad_a1 = grad_z2.dot(W2.T)\n",
    "    grad_z1 = grad_a1 * (z1 > 0)  # ReLU gradient\n",
    "    grad_W1 = X_train.T.dot(grad_z1)\n",
    "    grad_b1 = np.sum(grad_z1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    b1 -= learning_rate * grad_b1\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    b2 -= learning_rate * grad_b2\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    b3 -= learning_rate * grad_b3\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Validation\n",
    "z1_val = X_val.dot(W1) + b1\n",
    "a1_val = np.maximum(0, z1_val)\n",
    "z2_val = a1_val.dot(W2) + b2\n",
    "a2_val = np.maximum(0, z2_val)\n",
    "z3_val = a2_val.dot(W3) + b3\n",
    "predictions_val = z3_val\n",
    "val_loss = np.mean((predictions_val - y_val) ** 2)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 1)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

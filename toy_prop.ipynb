{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# network goes R50 --> R20 --> R20 --> R10\n",
    "w_1 = np.random.normal(0,0.05,(50, 20))\n",
    "b_1 = np.random.normal(0,0.05,20)\n",
    "\n",
    "w_2 = np.random.normal(0,0.05,(20, 10))\n",
    "b_2 = np.random.normal(0,0.05,10)\n",
    "\n",
    "w_3 = np.random.normal(0,0.05,(10, 1))\n",
    "b_3 = np.random.normal(0,0.05,1)\n",
    "\n",
    "X = np.random.normal(0, 1, (10000, 50))\n",
    "y = np.array([np.sum(X[x,:]**2) for x in range(10000)])\n",
    "\n",
    "X_train = X[:8000, :]\n",
    "X_test = X[8000:, :]\n",
    "\n",
    "y_train = np.sum(X[:8000, :], axis=1)\n",
    "y_test = np.sum(X[8000:, :], axis=1)\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return np.sum(.5*(y-y_pred)**2, axis=1)\n",
    "\n",
    "def dloss(y, y_pred):\n",
    "    return np.sum(y-y_pred, axis=1)\n",
    "\n",
    "def activation(z):\n",
    "    return z * (z>0)\n",
    "\n",
    "def dactivation(z):\n",
    "    return 1 * (z>0)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "def forward(X, params):\n",
    "    w_1, b_1, w_2, b_2, w_3, b_3 = params\n",
    "    z_1 = X @ w_1 + b_1\n",
    "    a_1 = activation(z_1)\n",
    "\n",
    "    z_2 = a_1 @ w_2 + b_2\n",
    "    a_2 = activation(z_2)\n",
    "\n",
    "    z_3 = a_2 @ w_3 + b_3\n",
    "    a_3 = activation(z_3)\n",
    "\n",
    "    return a_3, a_2, a_1, z_3, z_2, z_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for _ in range(epochs):    \n",
    "    a_3, a_2, a_1, z_3, z_2, z_1 = forward(X_train, (w_1, b_1, w_2, b_2, w_3, b_3))\n",
    "    out_test, _, _, _, _, _ = forward(X_test, (w_1, b_1, w_2, b_2, w_3, b_3))\n",
    "    l = loss(y_train, a_3)\n",
    "    dl_da_3 = dloss(y_train, a_3) # 8000 x 1\n",
    "    \n",
    "    da_3_dz_3 = dactivation(z_3) # 8000 x 1\n",
    "    dz_3_dw_3 = a_2 # 8000 x 10 x 1; to get z_3 we do a_2 (8000x10) @ w_3 (10 x 1) result is (8000x1)\n",
    "    dz_3_db_3 = np.ones((z_3.shape)) # 8000 x 1\n",
    "\n",
    "    dl_dw_3 = dl_da_3 * da_3_dz_3 * dz_3_dw_3\n",
    "    dl_dwb_3 = dl_da_3 * da_3_dz_3 * dz_3_db_3\n",
    "\n",
    "    da_2_dz_2 = \n",
    "\n",
    "\n",
    "\n",
    "    test_losses.append(np.mean(loss(y_test, out_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = np.random.normal(0,0.05,(3,2))\n",
    "b_1 = np.random.normal(0,0.05,3)\n",
    "\n",
    "w_2 = np.random.normal(0,0.05,(2,3))\n",
    "b_2 = np.random.normal(0,0.05,2)\n",
    "\n",
    "X = np.random.normal(0, 1, (10000, 2))\n",
    "y = np.array([np.sum(X[x,:]**2) for x in range(10000)])\n",
    "\n",
    "X_train = X[:8000, :]\n",
    "X_test = X[8000:, :]\n",
    "\n",
    "y_train = np.sum(X[:8000, :], axis=1)\n",
    "y_test = np.sum(X[8000:, :], axis=1)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return np.sum(.5*(y-y_pred)**2)\n",
    "\n",
    "def dloss(y, y_pred):\n",
    "    return np.sum(y-y_pred)\n",
    "\n",
    "def activation(z):\n",
    "    return z * (z>0)\n",
    "\n",
    "def dactivation(z):\n",
    "    return 1 * (z>0)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "def forward(X, params):\n",
    "    w_1, b_1, w_2, b_2 = params\n",
    "    z_1 = X @ w_1 + b_1\n",
    "    a_1 = activation(z_1)\n",
    "\n",
    "    z_2 = a_1 @ w_2 + b_2\n",
    "    a_2 = activation(z_2)\n",
    "\n",
    "    return a_2, a_1, z_2, z_1\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for i in range(8000):\n",
    "        # dC/W = dC/da * da/dz * dz/dW\n",
    "        \n",
    "        # Compute dC/da(L) and pass this to the last layers backward\n",
    "        # loop:\n",
    "        # 1) compute da(l)/dz(l) and matrix multiply with the passed param\n",
    "        # 2) compute dz(l)/dW(l) where W is the flattened gradient matrix\n",
    "        # 3) matrix multiply 2) with 1) to get the weight gradient\n",
    "        # 4) compute dz(l)/db(l) and multiply with 1) to get the bias gradient\n",
    "        # 5) apply weight and bias gradients\n",
    "        # 6) compute dz(l)/da(l-1) and matrix multiply with 1)\n",
    "        # 7) return 6) to be passed to the next layer\n",
    "\n",
    "        # dC/da(L) * da(L)/dz(L) \n",
    "        # z = Wa(l-1) + b\n",
    "        # a(l-1) = f(W(l-1)a(l-2) + b(l-2))\n",
    "        # dC/dW(L-1) = dC/da(L) * da(L)/dz(L) * dz(l)/da(l-1) * da(l-1)/dz(l-1) * dz(l-1)/dW(l-1)\n",
    "        \n",
    "        a_2, a_1, z_2, z_1 = forward(X_train, (w_1, b_1, w_2, b_2))\n",
    "        out_test, _, _, _, = forward(X_test, (w_1, b_1, w_2, b_2))\n",
    "        l = loss(y_train, a_2)\n",
    "        dl_da_2 = dloss(y_train, a_2) # dC/da\n",
    "\n",
    "        da_2_dz_2 = dactivation(z_2,) # da/dz\n",
    "        dz_2_dw_2 = a_2\n",
    "        dz_2_db_2 = np.ones((z_2.shape))\n",
    "\n",
    "        weight_factor_1 = dl_da_2 @ da_2_dz_2 @ dz_2_dw_2\n",
    "        bias_factor_1 = dl_da_2 @ da_2_dz_2 @ dz_2_db_2\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
